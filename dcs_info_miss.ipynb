{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pickle\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Out of the 4300~ sentences from lrec_ebm.csv\n",
    "<br>\n",
    "176 failed.\n",
    "<br>\n",
    "\n",
    "These 3 files from the 176 need manual intervention\n",
    "<br>\n",
    "\n",
    "344034.p\n",
    "<br>\n",
    "394434.p\n",
    "<br>\n",
    "420219.p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just change the filenames\n",
    "\n",
    "graphml_file = '344034.graphml'\n",
    "pickle_file = '344034.p'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "BASE_DIR = os.getcwd()\n",
    "PICKLE_DCS_FILE_NAME = os.path.join(BASE_DIR,'data','dcs_miss',pickle_file)\n",
    "GRAPHML_FILE_NAME = os.path.join(BASE_DIR,'data','dcs_miss',graphml_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining class for pickle file \n",
    "\n",
    "class DCS:\n",
    "    def __init__(self, sent_id, sentence):\n",
    "        self.sent_id = sent_id\n",
    "        self.sentence = sentence\n",
    "        self.dcs_chunks = []\n",
    "        self.lemmas = []\n",
    "        self.cng = []\n",
    "\n",
    "dcs_class_params = ['sentence id', 'sentence ', 'chunks', 'lemmas', 'morph class']        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Double characters mapping to single characters\n",
    "_dbl = dict({\n",
    "    'ai' : 'E',\n",
    "    'au' : 'O',\n",
    "    'kh' : 'K',\n",
    "    'gh' : 'G',\n",
    "    'ch' : 'C',\n",
    "    'jh' : 'J',\n",
    "    'ṭh' : 'W',\n",
    "    'ḍh' : 'Q',\n",
    "    'th' : 'T',\n",
    "    'dh' : 'D',\n",
    "    'ph' : 'P',\n",
    "    'bh' : 'B'})\n",
    "\n",
    "# One to one mapping\n",
    "_oth = dict({\n",
    "    'ā' : 'A',\n",
    "    'ī' : 'I',\n",
    "    'ū' : 'U',\n",
    "    'ṛ' : 'f',\n",
    "    'ṝ' : 'F',\n",
    "    'ḷ' : 'x',\n",
    "    'ḹ' : 'X',\n",
    "    'ṃ' : 'M',\n",
    "    'ḥ' : 'H',\n",
    "    'ṁ' : '~',\n",
    "    'ṅ' : 'N',\n",
    "    'ñ' : 'Y',\n",
    "    'ṭ' : 'w',\n",
    "    'ḍ' : 'q',\n",
    "    'ṇ' : 'R',\n",
    "    'ś' : 'S',\n",
    "    'ṣ' : 'z'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##IAST to SLP1 transliteration\n",
    "\n",
    "\n",
    "def iast2slp(src):\n",
    "    '''\n",
    "    Converts International Alphabet for Sanskrit Transliteration (IAST) scheme to\n",
    "    Sanskrit Library Phonetic Basic notation\n",
    "    '''\n",
    "    tgt = ''\n",
    "    inc = 0\n",
    "    while inc < len(src):\n",
    "        now = src[inc]\n",
    "        nxt = src[inc+1] if inc < len(src) - 1 else ''\n",
    "        if now + nxt in _dbl:\n",
    "            tgt += _dbl[now + nxt]\n",
    "            inc += 1\n",
    "        elif now in _oth:\n",
    "            tgt += _oth[now]\n",
    "        else:\n",
    "            tgt += now\n",
    "        inc += 1\n",
    "    return tgt\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the pickle file containing ground truth data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def pickle_data_give(filename):\n",
    "    '''Returns the attributes of the DCS pickle file\n",
    "    \n",
    "    Args:\n",
    "    filename (str) = name of file\n",
    "    \n",
    "    Returns:\n",
    "    output_load.sent_id (int): sentence id\n",
    "    output_load.sentence (str): sentence \n",
    "    output_load.dcs_chunks (list): chunks\n",
    "    output_load.lemmas (list): lemmas\n",
    "    output_load.cng (list): morphological class\n",
    "    \n",
    "    Raises:\n",
    "    '''\n",
    "    \n",
    "    output_load = pickle.load(open(filename, \"rb\"), encoding='utf-8')\n",
    "    \n",
    "    \n",
    "    return output_load.sent_id, output_load.sentence, output_load.dcs_chunks, output_load.lemmas, output_load.cng\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read graph ml file\n",
    "\n",
    "graph_file = nx.read_graphml(GRAPHML_FILE_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recursively break and simplify a nested list\n",
    "\n",
    "def simplify_nested_list(nested_list):   \n",
    "    \n",
    "    # requires resetting of \"all_single_elements_list\" variable prior to each function call\n",
    "    \n",
    "    if len(nested_list) >= 1 and type(nested_list) is list:\n",
    "        for idx, ele in enumerate(nested_list):\n",
    "            #print(idx, ele, type(ele))\n",
    "            if type(ele) is str:\n",
    "                \n",
    "                all_single_elements_list.append([ele])\n",
    "                continue\n",
    "                \n",
    "            else:\n",
    "                simplify_nested_list(nested_list[idx])\n",
    "                \n",
    "        \n",
    "    else:\n",
    "        \n",
    "        all_single_elements_list.append(nested_list)\n",
    "        return (nested_list)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================\n",
      "\n",
      "\n",
      "lemmas:\t[['bahu'], ['yuj']] \n",
      "cngs:\t[['101'], ['-190']]\n",
      "\n",
      "\n",
      "******************************\n",
      "\n",
      "\n",
      "Current lemma: ['bahu'] \tCurrent cng:['101']\n",
      "checking node 21\n",
      "\t\tLemma:ad\tCng:30\n",
      "\n",
      "checking node 19\n",
      "\t\tLemma:ad\tCng:71\n",
      "\n",
      "checking node 28\n",
      "\t\tLemma:varjita\tCng:30\n",
      "\n",
      "checking node 27\n",
      "\t\tLemma:ala\tCng:3\n",
      "\n",
      "checking node 22\n",
      "\t\tLemma:na\tCng:2\n",
      "\n",
      "checking node 11\n",
      "\t\tLemma:yukta\tCng:169\n",
      "\n",
      "checking node 17\n",
      "\t\tLemma:anna\tCng:3\n",
      "\n",
      "checking node 4\n",
      "\t\tLemma:yukta\tCng:29\n",
      "\n",
      "checking node 10\n",
      "\t\tLemma:yukta\tCng:171\n",
      "\n",
      "checking node 2\n",
      "\t\tLemma:bahu\tCng:101\n",
      "\n",
      "\t\t\t\t\t|$| - Here in node 2\n",
      "checking node 3\n",
      "\t\tLemma:bahu\tCng:100\n",
      "\n",
      "checking node 13\n",
      "\t\tLemma:eka\tCng:30\n",
      "\n",
      "checking node 29\n",
      "\t\tLemma:vfj\tCng:-190\n",
      "\n",
      "checking node 24\n",
      "\t\tLemma:viMSati\tCng:170\n",
      "\n",
      "checking node 33\n",
      "\t\tLemma:i\tCng:-42\n",
      "\n",
      "checking node 5\n",
      "\t\tLemma:yuj\tCng:-190\n",
      "\n",
      "checking node 26\n",
      "\t\tLemma:mala\tCng:3\n",
      "\n",
      "checking node 20\n",
      "\t\tLemma:ad\tCng:31\n",
      "\n",
      "checking node 12\n",
      "\t\tLemma:yuj\tCng:-190\n",
      "\n",
      "checking node 30\n",
      "\t\tLemma:varjita\tCng:101\n",
      "\n",
      "checking node 25\n",
      "\t\tLemma:a\tCng:3\n",
      "\n",
      "checking node 15\n",
      "\t\tLemma:eka\tCng:79\n",
      "\n",
      "checking node 1\n",
      "\t\tLemma:bahu\tCng:99\n",
      "\n",
      "checking node 9\n",
      "\t\tLemma:yukta\tCng:36\n",
      "\n",
      "checking node 8\n",
      "\t\tLemma:yukta\tCng:76\n",
      "\n",
      "checking node 14\n",
      "\t\tLemma:eka\tCng:3\n",
      "\n",
      "checking node 18\n",
      "\t\tLemma:ad\tCng:29\n",
      "\n",
      "checking node 32\n",
      "\t\tLemma:vfj\tCng:-190\n",
      "\n",
      "checking node 7\n",
      "\t\tLemma:yukta\tCng:35\n",
      "\n",
      "checking node 16\n",
      "\t\tLemma:eka\tCng:70\n",
      "\n",
      "checking node 31\n",
      "\t\tLemma:varjita\tCng:99\n",
      "\n",
      "checking node 23\n",
      "\t\tLemma:viMSati\tCng:90\n",
      "\n",
      "checking node 6\n",
      "\t\tLemma:yukta\tCng:75\n",
      "\n",
      "\n",
      "\n",
      "******************************\n",
      "\n",
      "\n",
      "Current lemma: ['yuj'] \tCurrent cng:['-190']\n",
      "checking node 21\n",
      "\t\tLemma:ad\tCng:30\n",
      "\n",
      "checking node 19\n",
      "\t\tLemma:ad\tCng:71\n",
      "\n",
      "checking node 28\n",
      "\t\tLemma:varjita\tCng:30\n",
      "\n",
      "checking node 27\n",
      "\t\tLemma:ala\tCng:3\n",
      "\n",
      "checking node 22\n",
      "\t\tLemma:na\tCng:2\n",
      "\n",
      "checking node 11\n",
      "\t\tLemma:yukta\tCng:169\n",
      "\n",
      "checking node 17\n",
      "\t\tLemma:anna\tCng:3\n",
      "\n",
      "checking node 4\n",
      "\t\tLemma:yukta\tCng:29\n",
      "\n",
      "checking node 10\n",
      "\t\tLemma:yukta\tCng:171\n",
      "\n",
      "checking node 2\n",
      "\t\tLemma:bahu\tCng:101\n",
      "\n",
      "checking node 3\n",
      "\t\tLemma:bahu\tCng:100\n",
      "\n",
      "checking node 13\n",
      "\t\tLemma:eka\tCng:30\n",
      "\n",
      "checking node 29\n",
      "\t\tLemma:vfj\tCng:-190\n",
      "\n",
      "checking node 24\n",
      "\t\tLemma:viMSati\tCng:170\n",
      "\n",
      "checking node 33\n",
      "\t\tLemma:i\tCng:-42\n",
      "\n",
      "checking node 5\n",
      "\t\tLemma:yuj\tCng:-190\n",
      "\n",
      "\t\t\t\t\t|$| - Here in node 5\n",
      "checking node 26\n",
      "\t\tLemma:mala\tCng:3\n",
      "\n",
      "checking node 20\n",
      "\t\tLemma:ad\tCng:31\n",
      "\n",
      "checking node 12\n",
      "\t\tLemma:yuj\tCng:-190\n",
      "\n",
      "\t\t\t\t\t|$| - Here in node 12\n",
      "checking node 30\n",
      "\t\tLemma:varjita\tCng:101\n",
      "\n",
      "checking node 25\n",
      "\t\tLemma:a\tCng:3\n",
      "\n",
      "checking node 15\n",
      "\t\tLemma:eka\tCng:79\n",
      "\n",
      "checking node 1\n",
      "\t\tLemma:bahu\tCng:99\n",
      "\n",
      "checking node 9\n",
      "\t\tLemma:yukta\tCng:36\n",
      "\n",
      "checking node 8\n",
      "\t\tLemma:yukta\tCng:76\n",
      "\n",
      "checking node 14\n",
      "\t\tLemma:eka\tCng:3\n",
      "\n",
      "checking node 18\n",
      "\t\tLemma:ad\tCng:29\n",
      "\n",
      "checking node 32\n",
      "\t\tLemma:vfj\tCng:-190\n",
      "\n",
      "checking node 7\n",
      "\t\tLemma:yukta\tCng:35\n",
      "\n",
      "checking node 16\n",
      "\t\tLemma:eka\tCng:70\n",
      "\n",
      "checking node 31\n",
      "\t\tLemma:varjita\tCng:99\n",
      "\n",
      "checking node 23\n",
      "\t\tLemma:viMSati\tCng:90\n",
      "\n",
      "checking node 6\n",
      "\t\tLemma:yukta\tCng:75\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# algorithm to find the cng+lemma combinations\n",
    "\n",
    "_, dcs_sentence, _, lemmas, cng = pickle_data_give(PICKLE_DCS_FILE_NAME)\n",
    "#print(lemma, '\\n',cng)\n",
    "\n",
    "#print(len(dcs_sentence.strip().rstrip().split(' ')))\n",
    "#print(dcs_sentence.strip().rstrip().split(' '))\n",
    "graph_ground_truth_ids = []\n",
    "flattened_chunk_no = []\n",
    "print('==========================================================================')\n",
    "\n",
    "\n",
    "#get chunk no's flattened list\n",
    "for idx, lemma in enumerate(lemmas):\n",
    "    #print(len(lemma))\n",
    "    if len(lemma)>1:\n",
    "        #print('$')\n",
    "        tmp=[]\n",
    "        for x in range(len(lemma)):\n",
    "            tmp.append(str(idx+1))\n",
    "        flattened_chunk_no.append(tmp)\n",
    "    else:\n",
    "        #print('%')\n",
    "        flattened_chunk_no.append(str(idx+1))\n",
    "        \n",
    "\n",
    "all_single_elements_list = []\n",
    "simplify_nested_list(flattened_chunk_no)\n",
    "flattened_chunk_no = all_single_elements_list \n",
    "        \n",
    "        \n",
    "all_single_elements_list = []\n",
    "simplify_nested_list(lemmas)\n",
    "lemmas = all_single_elements_list\n",
    "\n",
    "\n",
    "all_single_elements_list = []\n",
    "simplify_nested_list(cng)\n",
    "cng = all_single_elements_list\n",
    "\n",
    "print('\\n\\nlemmas:\\t%s \\ncngs:\\t%s' %(lemmas, cng))\n",
    "\n",
    "lemma_cng = zip(lemmas, cng)\n",
    "\n",
    "number_of_chunks = len(dcs_sentence.strip().rstrip().split(' '))\n",
    "\n",
    "\n",
    "for dcs_lemma, dcs_cng in lemma_cng:\n",
    "    print('\\n\\n******************************')\n",
    "    print('\\n\\nCurrent lemma: %s \\tCurrent cng:%s' %(dcs_lemma, dcs_cng))\n",
    "    \n",
    "    same_ground_truth_ids = []\n",
    "    \n",
    "    for di in graph_file.nodes():\n",
    "        \n",
    "        graph_file.add_node(str(di), ground_truth_id=0)\n",
    "        \n",
    "        # this is random one time traversal search in the whole graph\n",
    "        #print('Current node:%s' %(di))\n",
    "        temp_dict = graph_file.nodes[di]\n",
    "        \n",
    "        # convert the lemma to slp1\n",
    "        dcs_lemma[0] = iast2slp(dcs_lemma[0])\n",
    "        \n",
    "        \n",
    "        \n",
    "        print('checking node %s' %(di))\n",
    "        print('\\t\\tLemma:%s\\tCng:%s\\n' %(temp_dict['lemma'], temp_dict['cng']))\n",
    "        \n",
    "        # check if in the current node, the cng matches to the pickle file cng\n",
    "        if temp_dict['cng'] == int(dcs_cng[0]):\n",
    "                       \n",
    "            \n",
    "            \n",
    "            # check if in the current node, the lemma matches to the pickle file lemma\n",
    "            if (temp_dict['lemma'] == dcs_lemma[0]):\n",
    "                \n",
    "                # only if cng and the converted lemma match, then these are executed\n",
    "                \n",
    "                #if tempdict chunk number matches dcs chnk number\n",
    "                print('\\t\\t\\t\\t\\t|$| - Here in node %s' %(di))\n",
    "                #graph_file.add_node(di, ground_truth_id=1)\n",
    "                same_ground_truth_ids.append(di)\n",
    "        else:\n",
    "            \n",
    "            graph_file.add_node(str(di), ground_truth_id=0)\n",
    "        \n",
    "    graph_ground_truth_ids.append(same_ground_truth_ids)\n",
    "                \n",
    "                #print('#', di, '\\n')\n",
    "    \n",
    "\n",
    "#return len(graph_ground_truth_ids)/number_of_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function is used to pick one cng+lemma combination if redundant units are present.\n",
    "\n",
    "def pick_one_cng_lemma(graph_ground_truth_ids):\n",
    "    \n",
    "    update_ground_truth_ids = []\n",
    "    graph_ground_truth_ids = list(filter(None, graph_ground_truth_ids))\n",
    "    for ids_list in graph_ground_truth_ids:\n",
    "        if len(ids_list) > 1:\n",
    "            chose_cng_lemma = random.choice(ids_list)\n",
    "            graph_file.add_node(chose_cng_lemma, ground_truth_id=1)\n",
    "        else:\n",
    "            chose_cng_lemma = ids_list[0]            \n",
    "            graph_file.add_node(chose_cng_lemma, ground_truth_id=1)\n",
    "            \n",
    "        update_ground_truth_ids.append(chose_cng_lemma)\n",
    "    \n",
    "    return update_ground_truth_ids\n",
    "    \n",
    "graph_ground_truth_ids = pick_one_cng_lemma(graph_ground_truth_ids)\n",
    "#print(graph_ground_truth_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph ground truth nodes:\n",
      "['2', '12']\n",
      "bahuBiH yukte \n",
      "\n",
      "\n",
      "DCS sentence below:\n",
      "\n",
      "sentence id: \t344034\n",
      "sentence : \tbahuBiryukta ekAnnaviMSatyAmalavarjitEH    \n",
      "chunks: \t['bahu', 'yuj']\n",
      "lemmas: \t[['bahu', 'yuj']]\n",
      "morph class: \t[['101', '-190']]\n"
     ]
    }
   ],
   "source": [
    "# snippet to get the ground truth sentence from the graphML file\n",
    "\n",
    "print('Graph ground truth nodes:')\n",
    "graph_ground_truth_ids = sorted(graph_ground_truth_ids, key = lambda x : graph_file.nodes[x]['chunk_no'])\n",
    "print(graph_ground_truth_ids)\n",
    "\n",
    "\n",
    "for ids in graph_ground_truth_ids:\n",
    "    ids = str(ids)\n",
    "    print(graph_file.nodes[str(ids)]['word'], end=' ')\n",
    "\n",
    "  \n",
    "print('\\n\\n')\n",
    "print('DCS sentence below:\\n')\n",
    "\n",
    "for idx, _ in enumerate(pickle_data_give(PICKLE_DCS_FILE_NAME)):\n",
    "    print('%s: \\t%s' %(dcs_class_params[idx], _))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***The issue in the 3 dcs pickle files is that they dont have all lemma+cng information.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
